[{"content":"\rProject Overview A scalable, highly available 3-tier web application deployed on AWS infrastructure, featuring automated CI/CD pipelines and infrastructure as code. This project demonstrates modern cloud architecture patterns, DevOps best practices, and comprehensive automation.\nLive Demo: Project Link | GitHub: Repository\nArchitecture High-Level Design The application follows a classic 3-tier architecture pattern:\nPresentation Tier: Static website hosted on Amazon S3, accessible via CloudFront Application Tier: EC2 instances behind Application Load Balancer with Auto Scaling, plus AWS Lambda for serverless backend processing Data Tier: Amazon DynamoDB for NoSQL data storage with VPC Gateway Endpoint for secure access Key Components Amazon VPC: Isolated network environment with public and private subnets across multiple Availability Zones Application Load Balancer: Distributes traffic across EC2 instances and Lambda functions Auto Scaling Group: Automatically adjusts capacity based on demand AWS Lambda: Serverless compute for backend logic and DynamoDB interactions Amazon DynamoDB: Fast, scalable NoSQL database Amazon S3: Static website hosting with public access configuration IAM Roles \u0026amp; Policies: Fine-grained access control for all services Technical Implementation Infrastructure as Code Tool: Terraform\nResources Provisioned:\nVPC with public/private subnet architecture Security groups for ALB and EC2 instances Launch templates and Auto Scaling groups Application Load Balancer with target groups Lambda functions with execution roles DynamoDB tables S3 buckets for static hosting IAM roles and policies CI/CD Pipeline Implemented comprehensive GitHub Actions workflows for:\n1. Infrastructure Pipeline Automated Terraform deployment Infrastructure validation and planning State management in S3 backend 2. Frontend Pipeline Code quality analysis with SonarQube Automated build process CodeDeploy integration Deployment to EC2 Auto Scaling Group 3. Backend Pipeline SonarQube code scanning Quality gate enforcement Lambda function updates Rollback capabilities 4. Static Content Pipeline Direct S3 deployment Instant content updates CloudFront cache invalidation Key Features Scalability Auto Scaling based on CPU utilization Multi-AZ deployment for high availability Load balancing across multiple instances Security VPC isolation with private subnets Security groups with least privilege access IAM roles for service-to-service communication No hardcoded credentials Monitoring \u0026amp; Operations CloudWatch logs integration DynamoDB Gateway Endpoint for private connectivity Health checks on ALB Automated recovery mechanisms DevOps Best Practices Infrastructure as Code (Terraform) Automated testing and quality gates Continuous Integration/Continuous Deployment GitOps workflow Technologies Used Cloud Platform: Amazon Web Services (AWS)\nInfrastructure:\nAmazon EC2 Amazon VPC Application Load Balancer Auto Scaling Groups AWS Lambda Amazon DynamoDB Amazon S3 IaC \u0026amp; Automation:\nTerraform GitHub Actions AWS CodeDeploy Development Tools:\nPython (Lambda functions) Boto3 (AWS SDK) Git/GitHub Quality \u0026amp; Security:\nSonarQube AWS IAM Security Groups Implementation Approaches 1. Console-Based Deployment Manual configuration through AWS Management Console for learning and understanding each component.\n2. Terraform Automation Complete infrastructure provisioning using Terraform modules for reproducibility and version control.\n3. CI/CD Pipeline Fully automated deployment pipeline with multiple workflows for different application tiers.\nDeployment Workflow Developer ‚Üí Git Commit ‚Üí GitHub Actions ‚Üí Quality Gates ‚Üí AWS Deployment Pipeline Stages Code checkout and validation SonarQube quality analysis Build and test Infrastructure provisioning (if changed) Application deployment Health check verification Challenges \u0026amp; Solutions Challenge 1: Security Group Configuration Issue: Properly configuring security groups for ALB-EC2 communication Solution: Implemented security group rules allowing inbound traffic from ALB-SG to EC2-SG on port 80\nChallenge 2: Lambda-DynamoDB Access Issue: Lambda functions couldn\u0026rsquo;t access DynamoDB across VPC boundaries Solution: Implemented DynamoDB Gateway Endpoint for private, secure connectivity\nChallenge 3: Auto Scaling Integration Issue: New instances not automatically registering with ALB Solution: Configured Auto Scaling Group to directly integrate with Target Group\nKey Learnings Designing for high availability and fault tolerance Implementing infrastructure as code best practices Building secure, isolated network architectures Creating automated CI/CD pipelines Managing AWS resources at scale Implementing quality gates in deployment workflows Future Enhancements Implement AWS CloudFront for global content delivery Add AWS WAF for web application firewall protection Integrate AWS Secrets Manager for credential management Implement blue-green deployment strategy Add comprehensive monitoring with CloudWatch dashboards Implement automated backup and disaster recovery Add API Gateway for RESTful API management Project Metrics Infrastructure Components: 15+ AWS services Deployment Time: ~10 minutes (fully automated) Uptime: 99.9% availability across multiple AZs Scalability: Auto-scales from 2 to 10 instances based on load Repository Structure ‚îú‚îÄ‚îÄ Terraform/\r‚îÇ ‚îú‚îÄ‚îÄ main.tf\r‚îÇ ‚îú‚îÄ‚îÄ variables.tf\r‚îÇ ‚îú‚îÄ‚îÄ backend.tf\r‚îÇ ‚îî‚îÄ‚îÄ modules/\r‚îú‚îÄ‚îÄ Static-content/\r‚îÇ ‚îú‚îÄ‚îÄ index.html\r‚îÇ ‚îú‚îÄ‚îÄ background.jpeg\r‚îÇ ‚îî‚îÄ‚îÄ logo.jpeg\r‚îú‚îÄ‚îÄ scripts/\r‚îÇ ‚îî‚îÄ‚îÄ user-data.sh\r‚îî‚îÄ‚îÄ .github/\r‚îî‚îÄ‚îÄ workflows/\r‚îú‚îÄ‚îÄ infrastructure.yml\r‚îú‚îÄ‚îÄ frontend.yml\r‚îú‚îÄ‚îÄ backend.yml\r‚îî‚îÄ‚îÄ static.yml Contact \u0026amp; Contributions This project demonstrates practical experience with AWS cloud architecture, infrastructure automation, and DevOps practices. I\u0026rsquo;m open to discussing the technical decisions, architecture patterns, and implementation details.\nConnect with me: LinkedIn | GitHub | Email\nLast Updated: November 2024\n","permalink":"http://localhost:1313/projects/3-tier-web-application/","summary":"\u003c!-- # 3-Tier Web Application on AWS --\u003e\r\n\u003ch2 id=\"project-overview\"\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eA scalable, highly available 3-tier web application deployed on AWS infrastructure, featuring automated CI/CD pipelines and infrastructure as code. This project demonstrates modern cloud architecture patterns, DevOps best practices, and comprehensive automation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLive Demo:\u003c/strong\u003e \u003ca href=\"#\"\u003eProject Link\u003c/a\u003e | \u003cstrong\u003eGitHub:\u003c/strong\u003e \u003ca href=\"#\"\u003eRepository\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"architecture\"\u003eArchitecture\u003c/h2\u003e\n\u003ch3 id=\"high-level-design\"\u003eHigh-Level Design\u003c/h3\u003e\n\u003cp\u003eThe application follows a classic 3-tier architecture pattern:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePresentation Tier\u003c/strong\u003e: Static website hosted on Amazon S3, accessible via CloudFront\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eApplication Tier\u003c/strong\u003e: EC2 instances behind Application Load Balancer with Auto Scaling, plus AWS Lambda for serverless backend processing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData Tier\u003c/strong\u003e: Amazon DynamoDB for NoSQL data storage with VPC Gateway Endpoint for secure access\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"key-components\"\u003eKey Components\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eAmazon VPC\u003c/strong\u003e: Isolated network environment with public and private subnets across multiple Availability Zones\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eApplication Load Balancer\u003c/strong\u003e: Distributes traffic across EC2 instances and Lambda functions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAuto Scaling Group\u003c/strong\u003e: Automatically adjusts capacity based on demand\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAWS Lambda\u003c/strong\u003e: Serverless compute for backend logic and DynamoDB interactions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAmazon DynamoDB\u003c/strong\u003e: Fast, scalable NoSQL database\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAmazon S3\u003c/strong\u003e: Static website hosting with public access configuration\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIAM Roles \u0026amp; Policies\u003c/strong\u003e: Fine-grained access control for all services\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"technical-implementation\"\u003eTechnical Implementation\u003c/h2\u003e\n\u003ch3 id=\"infrastructure-as-code\"\u003eInfrastructure as Code\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTool\u003c/strong\u003e: Terraform\u003c/p\u003e","title":"3-Tier Web Application on AWS"},{"content":"Bejadi Rajesh Reddy Malla Reddy University, Hyderabad\nüìß Email: bejadirajeshreddy@gmail.com\nüì± Mobile: +91 6305597440\nüîó LinkedIn: https://www.linkedin.com/in/rajeshreddybejadi/\nüíª GitHub: https://github.com/BejadiRajeshReddy\nProfessional Summary Motivated IT professional with knowledge in application support, system monitoring, and troubleshooting. Hands-on exposure to AWS, Linux, Docker, Kubernetes, and networking basics. Strong problem-solving mindset with the ability to assist users, resolve incidents, and maintain smooth IT operations.\nOpen to Support/Monitoring, IT Operations, or Cloud/DevOps-related roles, with a commitment to learn and grow while contributing to organizational success.\nEducation Qualification Institution Year Score BTech, Computer Science Malla Reddy University, Hyderabad 2020‚Äì2024 CGPA: 8.47/10 Class 12 Yadadri Junior College, Hyderabad 2018‚Äì2020 91.3% Class 10 Veda High School, Kondapak, Siddipet 2017‚Äì2018 95% Experience Web Developer ‚Äì ETE Digital Pvt Ltd, Bangalore Apr 2025 ‚Äì Present\nMaintaining and enhancing company‚Äôs main website (etedigital.com) ensuring responsiveness and performance. Monitoring application performance and resolving incidents related to APIs and deployments. Collaborated with backend/DevOps teams for bug fixes, integration issues, and support requests. Worked on log analysis, error resolution, and performance tuning to ensure uptime. Provided technical support to internal teams during sprints and client deliverables. AWS Cloud Intern ‚Äì F13 Technologies Apr 2023 ‚Äì Jul 2023\nWorked on AWS EC2, S3, IAM for deployments, access management, and system monitoring. Assisted in resolving infrastructure issues and supported CI/CD pipeline configurations. Created deployment documentation and contributed to incident resolution. Projects DockRails Pipeline ‚Äì End to End CI/CD Project (Mar 2025) Containerized Ruby on Rails + PostgreSQL application using Docker. Deployed application using Kubernetes with manifests, services, and persistent volumes. Automated CI/CD using Tekton Pipelines with custom tasks for build, push, deploy. Implemented GitOps with Argo CD for continuous delivery from GitHub to Kubernetes. Tech Stack: Docker, Kubernetes, Argo CD, Tekton, Helm, GitHub, Ruby on Rails, PostgreSQL. End to End CI/CD Project (Sep 2024) Built CI/CD pipeline using Jenkins. Created Dockerfiles for image creation and application deployment. Managed code versioning with Git; integrated SonarQube for code quality. Tech Stack: Linux, Jenkins, Maven, SonarQube, Git, GitHub, Docker, Trivy. Job Hub ‚Äì Job Portal (Students \u0026amp; Recruiters) (Apr 2025) Implemented signup, login, logout authentication system. Built job application system for students and job posting features for recruiters. Focused on reusable UI components and optimized state management. Tech Stack: React, Tailwind, Next.js, ShadCN. Technical Skills Support \u0026amp; Monitoring Linux, SonarQube, Git, CI/CD, Logging, Ticketing, Incident Handling\nCloud \u0026amp; Infrastructure AWS (EC2, S3, EBS, IAM, VPC), Google Cloud\nContainers \u0026amp; Orchestration Docker, Kubernetes, Jenkins, Argo CD, Tekton\nNetworking \u0026amp; Operating Systems IP, DNS, Firewalls, Ubuntu, Kali Linux, Windows\nProgramming \u0026amp; Scripting Python, JavaScript, MySQL, Bash\n","permalink":"http://localhost:1313/resume/","summary":"Rajesh Reddy Bejadi\u0026rsquo;s Resume","title":"Resume"},{"content":"\rToday‚Äôs focus was on one of the most important concepts in Terraform: Variables.\nVariables help make Terraform configurations reusable, flexible, and clean ‚Äî especially when working with AWS infrastructure.\nThis blog covers why variables are needed, the different types, how to declare them, and how Terraform decides which value takes priority when multiple sources exist.\nWhy Do We Need Variables in Terraform? As your AWS infrastructure grows, hard-coding values becomes messy and repetitive.\nVariables help solve this by allowing you to:\nReuse configs across environments Change values without editing code Keep your Terraform files clean Avoid duplication Separate configuration from infrastructure logic Terraform supports three major kinds of variables:\nInput variables Output variables Local values (locals) Let‚Äôs explore each.\n3. Local Values (locals) Locals are internal variables ‚Äî they don‚Äôt take input and aren‚Äôt printed as outputs.\nThey are just shortcuts to reduce repetition or combine values.\nExample: locals { environment = \u0026#34;dev\u0026#34; name = \u0026#34;${local.environment}-app\u0026#34; } Use it like:\ntags = { Name = local.name } Locals = cleaner code \u0026amp; less duplication.\nExample: Full Variable Workflow Files:\nvariables.tf\nvariable \u0026#34;region\u0026#34; { type = string default = \u0026#34;us-east-1\u0026#34; } terraform.tfvars\nregion = \u0026#34;us-west-2\u0026#34; CLI:\nterraform apply -var=\u0026#34;region=ap-south-1\u0026#34; Result:\nTerraform uses ap-south-1, because CLI \u0026gt; tfvars \u0026gt; default.\n","permalink":"http://localhost:1313/blogs/day-5-terraform/","summary":"\u003cdiv style=\"width: 100%;\"\u003e\r\n    \u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n          \u003ciframe allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen\" loading=\"eager\" referrerpolicy=\"strict-origin-when-cross-origin\" src=\"https://www.youtube.com/embed/QMsJholPkDY?autoplay=0\u0026amp;controls=1\u0026amp;end=0\u0026amp;loop=0\u0026amp;mute=0\u0026amp;start=0\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" title=\"YouTube video\"\u003e\u003c/iframe\u003e\n        \u003c/div\u003e\n\r\n\u003c/div\u003e\r\n\u003cbr\u003e\r\n\u003cp\u003eToday‚Äôs focus was on one of the most important concepts in Terraform: \u003cstrong\u003eVariables\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eVariables help make Terraform configurations reusable, flexible, and clean ‚Äî especially when working with AWS infrastructure.\u003c/p\u003e\n\u003cp\u003eThis blog covers why variables are needed, the different types, how to declare them, and how Terraform decides which value takes priority when multiple sources exist.\u003c/p\u003e","title":"Day 5 ‚Äî Understanding Terraform Variables (Input, Output, Locals \u0026 Precedence)"},{"content":"Terraform Structure Overview A Terraform project usually contains four important blocks:\nBlock Purpose terraform Declares providers and versions provider Connects Terraform to AWS / Cloud resource Creates infrastructure data Reads existing infrastructure Now let‚Äôs see each one with examples.\nüåç 1. Terraform Block The terraform block is used to configure settings that affect Terraform itself, not the cloud provider or the resources. The terraform block tells Terraform how to behave.\nIt does not create infrastructure ‚Äî it configures Terraform‚Äôs settings.\nYou usually place it at the top of your file.\nWhat it does: Defines required providers Pins provider versions Sets Terraform CLI version Configures backend (remote state) Example:\nterraform { required_version = \u0026#34;\u0026gt;= 1.5.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;my-tf-state-bucket\u0026#34; key = \u0026#34;terraform/state.tfstate\u0026#34; region = \u0026#34;ap-south-1\u0026#34; } } Components Explained: required_version Ensures Terraform is using the correct version. required_providers Tells Terraform which providers to download and their versions. backend Where the state file is stored (local by default, S3/DynamoDB for teams). üîç So what happens when you don‚Äôt add the block? Terraform will:\nAutomatically install the latest AWS provider version Use whatever version it finds suitable This is okay for learning or small tests, but dangerous for production.\n‚úÖ Why Terraform Works Without the terraform Block 1Ô∏è‚É£ Terraform Automatically Downloads the Required Provider If you write this:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-east-1\u0026#34; } and Terraform sees that you used:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; {} Terraform automatically understands that you need the AWS provider and downloads the latest compatible version.\nSo the terraform block is not mandatory.\n‚úÖ Then Why Do We Use the terraform { required_providers {} } Block? üëâ This block is used to control versions and source of providers, especially in production.\nExample:\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.23.0\u0026#34; } } } This ensures:\nSame provider version for everyone (Team members get consistent results) Avoids breaking changes (A future Terraform update won\u0026rsquo;t suddenly break your code) Explicit control (You know which version is being used) ‚òÅÔ∏è 2. Provider Block (Mandatory) This tells Terraform which cloud we are using.\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-south-1\u0026#34; } ‚úî Explanation required_providers: Ask Terraform to download AWS provider plugin. provider \u0026quot;aws\u0026quot;: Sets region and credentials (default uses aws configure). üöÄ 3. Resource Block (Creates Infrastructure) A resource = something Terraform will CREATE, UPDATE, DELETE. Examples:\nEC2 instance VPC S3 bucket IAM user RDS database Syntax:\nresource \u0026#34;\u0026lt;PROVIDER\u0026gt;_\u0026lt;SERVICE\u0026gt;\u0026#34; \u0026#34;\u0026lt;NAME\u0026gt;\u0026#34; { # arguments } Example:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;mybucket\u0026#34; { bucket = \u0026#34;my-tf-bucket-123\u0026#34; } ","permalink":"http://localhost:1313/blogs/day-6/","summary":"\u003ch2 id=\"terraform-structure-overview\"\u003e\u003cstrong\u003eTerraform Structure Overview\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eA Terraform project usually contains four important blocks:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eBlock\u003c/th\u003e\n          \u003cth\u003ePurpose\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eterraform\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eDeclares providers and versions\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eprovider\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eConnects Terraform to AWS / Cloud\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eresource\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eCreates infrastructure\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003edata\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eReads existing infrastructure\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNow let‚Äôs see each one with examples.\u003c/p\u003e\n\u003ch1 id=\"-1-terraform-block\"\u003eüåç 1. \u003cstrong\u003eTerraform Block\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eThe \u003cstrong\u003eterraform block\u003c/strong\u003e is used to configure settings that affect \u003cstrong\u003eTerraform itself\u003c/strong\u003e, not the cloud provider or the resources. The \u003cstrong\u003eterraform block\u003c/strong\u003e tells Terraform \u003cstrong\u003ehow to behave\u003c/strong\u003e.\u003c/p\u003e","title":"Day 6 "},{"content":"Terraform Structure Overview A Terraform project usually contains four important blocks:\nBlock Purpose terraform Declares providers and versions provider Connects Terraform to AWS / Cloud resource Creates infrastructure data Reads existing infrastructure Now let‚Äôs see each one with examples.\nüåç 1. Terraform Block The terraform block is used to configure settings that affect Terraform itself, not the cloud provider or the resources. The terraform block tells Terraform how to behave.\nIt does not create infrastructure ‚Äî it configures Terraform‚Äôs settings.\nYou usually place it at the top of your file.\nWhat it does: Defines required providers Pins provider versions Sets Terraform CLI version Configures backend (remote state) Example:\nterraform { required_version = \u0026#34;\u0026gt;= 1.5.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;my-tf-state-bucket\u0026#34; key = \u0026#34;terraform/state.tfstate\u0026#34; region = \u0026#34;ap-south-1\u0026#34; } } Components Explained: required_version Ensures Terraform is using the correct version. required_providers Tells Terraform which providers to download and their versions. backend Where the state file is stored (local by default, S3/DynamoDB for teams). üîç So what happens when you don‚Äôt add the block? Terraform will:\nAutomatically install the latest AWS provider version Use whatever version it finds suitable This is okay for learning or small tests, but dangerous for production.\n‚úÖ Why Terraform Works Without the terraform Block 1Ô∏è‚É£ Terraform Automatically Downloads the Required Provider If you write this:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;eu-east-1\u0026#34; } and Terraform sees that you used:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; {} Terraform automatically understands that you need the AWS provider and downloads the latest compatible version.\nSo the terraform block is not mandatory.\n‚úÖ Then Why Do We Use the terraform { required_providers {} } Block? üëâ This block is used to control versions and source of providers, especially in production.\nExample:\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.23.0\u0026#34; } } } This ensures:\nSame provider version for everyone (Team members get consistent results) Avoids breaking changes (A future Terraform update won\u0026rsquo;t suddenly break your code) Explicit control (You know which version is being used) ‚òÅÔ∏è 2. Provider Block (Mandatory) This tells Terraform which cloud we are using.\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 6.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-south-1\u0026#34; } ‚úî Explanation required_providers: Ask Terraform to download AWS provider plugin. provider \u0026quot;aws\u0026quot;: Sets region and credentials (default uses aws configure). üöÄ 3. Resource Block (Creates Infrastructure) A resource = something Terraform will CREATE, UPDATE, DELETE. Examples:\nEC2 instance VPC S3 bucket IAM user RDS database Syntax:\nresource \u0026#34;\u0026lt;PROVIDER\u0026gt;_\u0026lt;SERVICE\u0026gt;\u0026#34; \u0026#34;\u0026lt;NAME\u0026gt;\u0026#34; { # arguments } Example:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;mybucket\u0026#34; { bucket = \u0026#34;my-tf-bucket-123\u0026#34; } ","permalink":"http://localhost:1313/projects/day-6/","summary":"\u003ch2 id=\"terraform-structure-overview\"\u003e\u003cstrong\u003eTerraform Structure Overview\u003c/strong\u003e\u003c/h2\u003e\n\u003cp\u003eA Terraform project usually contains four important blocks:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eBlock\u003c/th\u003e\n          \u003cth\u003ePurpose\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eterraform\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eDeclares providers and versions\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eprovider\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eConnects Terraform to AWS / Cloud\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eresource\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eCreates infrastructure\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003edata\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eReads existing infrastructure\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNow let‚Äôs see each one with examples.\u003c/p\u003e\n\u003ch1 id=\"-1-terraform-block\"\u003eüåç 1. \u003cstrong\u003eTerraform Block\u003c/strong\u003e\u003c/h1\u003e\n\u003cp\u003eThe \u003cstrong\u003eterraform block\u003c/strong\u003e is used to configure settings that affect \u003cstrong\u003eTerraform itself\u003c/strong\u003e, not the cloud provider or the resources. The \u003cstrong\u003eterraform block\u003c/strong\u003e tells Terraform \u003cstrong\u003ehow to behave\u003c/strong\u003e.\u003c/p\u003e","title":"Day 6 "},{"content":"This is a sample blog post. You can use this as a template to create more blog posts.\nIntroduction Welcome to the sample blog! This is where you can write about your ideas, projects, or anything you want to share.\nContent Feel free to add images, code snippets, or any other content to make your blog engaging.\n# Example Python code print(\u0026#34;Hello, world!\u0026#34;) Conclusion Thank you for reading this sample blog. Stay tuned for more updates!\n","permalink":"http://localhost:1313/blogs/sample-blog/","summary":"\u003cp\u003eThis is a sample blog post. You can use this as a template to create more blog posts.\u003c/p\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eWelcome to the sample blog! This is where you can write about your ideas, projects, or anything you want to share.\u003c/p\u003e\n\u003ch2 id=\"content\"\u003eContent\u003c/h2\u003e\n\u003cp\u003eFeel free to add images, code snippets, or any other content to make your blog engaging.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e# Example Python code\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, world!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eThank you for reading this sample blog. Stay tuned for more updates!\u003c/p\u003e","title":"Day 5 ‚Äî Understanding Terraform Variables (Input, Output, Locals \u0026 Precedence)"}]